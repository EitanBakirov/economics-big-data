---
title: "big_data_final_task"
author: "Ariel&Eitan&Yuval"
date: "2024-10-05"
output: html_document
---

# Project: Predict Uber Demand in New York

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Load necessary libraries
```{r necessary_libraries}
library(dplyr)
library(geosphere) 
library(leaflet)
library(ggplot2)
library(lubridate)
library(readr)
library(stringr)
```

## Load the CSV files and over looking at it
```{r load_data}
train_raw_data <- read.csv("train_raw_data.csv")
train_filtered_2000m_data <- read.csv("train_raw_data_dists_more_then_2000.csv")
```

### train_raw_data.csv
train_raw_data.csv: Contains all Uber pickups in NYC between April 1 and September 9, 2014, without filtering by distance.

```{r raw_data_look}
# Show the first 5 rows of each train_raw_data
head(train_raw_data, 5)

# Summary statistics for the full dataset
summary(train_raw_data)
```
* Latitude (lat) ranges from 39.66 to 42.12, and Longitude (lon) ranges from -74.93 to -72.07, indicating the pickups occurred within the New York area.

* The median and mean values for both latitude (around 40.74) and longitude (around -73.97) show that most pickups took place near the central part of New York, close to the Empire State Building.

### train_raw_data_dists_more_then_2000.csv
train_raw_data_dists_more_then_2000.csv: Contains only Uber pickups outside a 2000-meter radius from the Empire State Building.

```{r filtered_data_look}
# Show the first 5 rows of each train_raw_data
head(train_filtered_2000m_data, 5)

# Summary statistics for the filtered dataset
summary(train_filtered_2000m_data)
```


* Similar latitude and longitude ranges, but these pickups occurred outside a 2000-meter radius from the Empire State Building.

* The dist column represents the distance from the Empire State Building, ranging from 2000 meters to 220,970 meters (about 221 km). The median distance is 4083 meters.

#### Features Explanation:
Datasets train_raw_data.csv and train_raw_data_dists_more_then_2000.csv features:

1. datetime: This column represents the date and time when the Uber pickup occurred.

2. lat: This column contains the latitude of the pickup location, indicating the geographical position (north-south axis) of the ride.

3. lon: This column contains the longitude of the pickup location, representing the geographical position (east-west axis) of the ride.

4. Base:  This column includes a code representing the TLC (Taxi and Limousine Commission) base company code affiliated with the Uber pickup. This code identifies the licensed dispatching base that managed the Uber trip. Every ride must be affiliated with a TLC-licensed base, which is responsible for dispatching the trip.(License Number).

5. dist (only for train_raw_data_dists_more_then_2000.csv): This column represents the distance (in meters) between the pickup location and the Empire State Building. All values in this dataset are greater than 2000 meters, as the dataset is filtered to include only pickups beyond this radius.

```{r interactive_map}
# Define latitude, longitude, and radius
lat <- 40.7484 # Empire State Building coordinates
lon <- -73.985 # Empire State Building coordinates

small_radius <- 1000  # Radius in meters
big_radius <- 2000  # Radius in meters

# Create the leaflet map
leaflet() %>%
  addTiles() %>%  # Add default OpenStreetMap tiles
  addMarkers(lng = lon, lat = lat, popup = "Empire State Building") %>%  # 
  addCircles(lng = lon, lat = lat, radius = small_radius, color = "blue", fillOpacity = 0.2) %>% # 
  addCircles(lng = lon, lat = lat, radius = big_radius, color = "red", fillOpacity = 0.1)  #
```

## Part A: Data Rearrangement

### Filtering Data
Only relevant for the train_raw_data.csv data:
```{r}
# Define Empire State Building coordinates
empire_state_coords <- c(-73.985, 40.7484)  # Longitude, Latitude

# Calculate the distance from each pickup to the Empire State Building
train_raw_data <- train_raw_data %>%
  mutate(dist = distHaversine(cbind(lon, lat), empire_state_coords))
```


#### Is 2000m is in train_raw?
```{r filtered_data_look}

# str(train_filtered_2000m_data)
# str(train_raw_data)
# 
# # Ensure both data frames have the same columns before comparison
# common_columns <- intersect(names(train_filtered_2000m_data), names(train_raw_data))
# 
# # Select only the common columns for comparison
# train_filtered_2000m_data_common <- train_filtered_2000m_data %>% select(all_of(common_columns))
# train_raw_data_common <- train_raw_data %>% select(all_of(common_columns))
# 
# # Now use setdiff to find rows in train_filtered_2000m_data not in train_raw_data
# missing_rows <- setdiff(train_filtered_2000m_data_common, train_raw_data_common)
# 
# # Check if any rows are missing
# if (nrow(missing_rows) == 0) {
#   print("All rows in train_filtered_2000m_data are present in train_raw_data.")
# } else {
#   print("Some rows in train_filtered_2000m_data are missing from train_raw_data.")
#   print(missing_rows)
# }

```
So, 2,623  rows in 2000m are unique - (not in train_raw_data)

#### Is 2000 above from train_data in 2000m?
```{r filtered_data_look}

# # Create a new dataframe with rows where dist is greater than or equal to 2000
# check_train_raw_above_2000 <- train_raw_data %>%
#   filter(dist >= 2000)
# 
# # Ensure both data frames have the same columns before comparison
# common_columns <- intersect(names(check_train_raw_above_2000), names(train_filtered_2000m_data))
# 
# # Select only common columns for comparison
# check_train_raw_above_2000_common <- check_train_raw_above_2000 %>% select(all_of(common_columns))
# train_filtered_2000m_data_common <- train_filtered_2000m_data %>% select(all_of(common_columns))
# 
# # Now use setdiff
# missing_rows <- setdiff(check_train_raw_above_2000_common, train_filtered_2000m_data_common)
# 
# # Check if any rows are missing
# if (nrow(missing_rows) == 0) {
#   print("All rows in check_train_raw_above_2000 are present in train_filtered_2000m_data.")
# } else {
#   print("Some rows in check_train_raw_above_2000 are missing from train_filtered_2000m_data.")
#   print(missing_rows)
# }
```


```{r}
# Add a new column 'distance_category' to classify the distance
train_raw_data <- train_raw_data %>%
  mutate(distance_category = ifelse(dist <= 1000, "<= 1000 meters", "> 1000 meters"))

ggplot(train_raw_data, aes(x = distance_category)) +
  geom_bar(fill = c("lightblue", "lightgreen")) +
  labs(title = "Count of Uber Pickups within and beyond 1000 meters",
       x = "Distance Category",
       y = "Count of Pickups") +
  theme_minimal()
```

We can see that there are many Uber pickups occurred beyond the 1000-meter radius that need to be filltered.

```{r filtering_data}
# Filter data to include only pickups within a 1000-meter radius and remove the 'distance_category' column
train_filtered_1000m_data <- train_raw_data %>%
  filter(dist <= 1000) %>%
  select(-distance_category)  # Remove the 'distance_category' column if it exists
```

Making sure the filltered worked:
```{r}
# Check the maximum distance in the filtered dataset
max_distance <- max(train_filtered_1000m_data$dist)

# Print the maximum distance
print(max_distance)
```
Let's check how many rows in this dataset we are left with:
```{r}
rows_1000m <- nrow(train_filtered_1000m_data)
print(paste("Number of rows in the 1000m dataset:", rows_1000m))
```
### Filtering by Time
We need to make sure the timestamp column is formatted correctly.
* The timestamp format (for example: 2014-04-01T00:02:00Z) includes a "T" between the date and time and ends with a "Z", which indicates that the time is in UTC (Coordinated Universal Time).
We need to adjust the format argument to handle this format. Additionally, we can specify the time zone using tz = "UTC" to make sure the conversion handles the UTC time correctly.
```{r}
# Ensure the 'timestamp' column is in POSIXct format, handling the 'T' and 'Z' characters
train_filtered_1000m_data <- train_filtered_1000m_data %>%
  mutate(timestamp = as.POSIXct(timestamp, format="%Y-%m-%dT%H:%M:%SZ", tz = "UTC"))

train_filtered_2000m_data <- train_filtered_2000m_data %>%
  mutate(timestamp = as.POSIXct(timestamp, format="%Y-%m-%dT%H:%M:%SZ", tz = "UTC"))
```

Visualize the distribution of Uber pickups by hour to check if there are any outside 17:00-00:00:
```{r}
plot_pickup_distribution <- function(data, title, color) {
  ggplot(data, aes(x = hour(timestamp))) +
    geom_bar(fill = color) +
    labs(title = title,
         x = "Hour of Day",
         y = "Count of Pickups") +
    scale_x_continuous(breaks = 0:23) +  # Set x-axis for hours (0 to 23)
    theme_minimal() +
    theme(axis.text.x = element_text(angle = 45, hjust = 1))  # Rotate x-axis labels for clarity
}
```
Lets say that each dataset will get a color: lightblue for the 1000m data and lightgreen for the 2000m data

```{r}
# For 1000-meter filtered data
plot_pickup_distribution(train_filtered_1000m_data, "Distribution of Uber Pickups by Hour (1000m Data)", "lightblue")
```

```{r}
# For 2000-meter filtered data
plot_pickup_distribution(train_filtered_2000m_data, "Distribution of Uber Pickups by Hour (2000m Data)", "lightgreen")
```
As seen in the two graphs, there are significant amounts of data outside the desired range of 17:00 to 00:00 for both datasets (1000m and 2000m data). Therefore, to focus on the data for pickups that occurred between 17:00 and 00:00, we will apply a filter to exclude the hours outside this range.

Filter data to include only pickups between 17:00 and 00:00:
```{r}
# For 1000-meter filtered data
train_filtered_1000m_data <- train_filtered_1000m_data %>%
  filter(hour(timestamp) >= 17 | hour(timestamp) == 0)

# For 2000-meter filtered data
train_filtered_2000m_data <- train_filtered_2000m_data %>%
  filter(hour(timestamp) >= 17 | hour(timestamp) == 0)
```

Check the Results:
```{r}
# Create a data frame with all hours from 0 to 23
hours <- data.frame(hour = 0:23)
```

```{r}
plot_hourly_pickup_distribution <- function(data, hours_df, title, color) {
  # Extract the hour and count the number of pickups per hour
  hourly_data <- data %>%
    mutate(hour = hour(timestamp)) %>%
    count(hour)
  
  # Merge the hourly counts with the 'hours' data frame to include missing hours as 0
  hourly_data_complete <- merge(hours_df, hourly_data, by = "hour", all.x = TRUE)
  hourly_data_complete[is.na(hourly_data_complete)] <- 0  # Replace NAs with 0
  
  # Plot the hourly data
  ggplot(hourly_data_complete, aes(x = hour, y = n)) +
    geom_bar(stat = "identity", fill = color) +
    labs(title = title,
         x = "Hour of Day",
         y = "Count of Pickups") +
    scale_x_continuous(breaks = 0:23) +  # Show all hours from 0 to 23 on x-axis
    theme_minimal()
}
```

```{r}
# Plot for 1000-meter filtered data (lightblue color)
plot_hourly_pickup_distribution(train_filtered_1000m_data, hours, 
                                "Filtered Uber Pickups by Hour (1000m Data, Expected: 17:00 to 00:00)", 
                                "lightblue")
```
```{r}
# Plot for 2000-meter filtered data (lightgreen color)
plot_hourly_pickup_distribution(train_filtered_2000m_data, hours, 
                                "Filtered Uber Pickups by Hour (2000m Data, Expected: 17:00 to 00:00)", 
                                "lightgreen")
```
We are left with only rows with the desired time range!

### Creating 15-Minute Intervals
We're grouping the `timestamp` data into 15-minute intervals to make it easier to spot patterns and trends in Uber pickups. This lets us count how many pickups happen in each time block, helping us better understand demand and predict future activity.

The function takes a dataset as input, creates the 15-minute time intervals, and calculates the pickup counts per interval:

```{r}
create_pickup_counts <- function(data) {
  pickup_counts <- data %>%
    mutate(time_interval = floor_date(timestamp, "15 minutes")) %>%
    group_by(time_interval) %>%
    summarise(number_of_pickups = n())
  
  return(pickup_counts)
}
```

```{r}
# Use the function for 1000m data
pickup_counts_1000m <- create_pickup_counts(train_filtered_1000m_data)
head(pickup_counts_1000m)
```

```{r}
# Use the function for 2000m data
pickup_counts_2000m <- create_pickup_counts(train_filtered_2000m_data)
head(pickup_counts_2000m)
```

```{r time_intervals}
create_time_intervals <- function(data, pickup_counts) {
  # Join the pickup counts back to the original data to include all columns
  data_with_intervals <- data %>%
    mutate(time_interval = floor_date(timestamp, "15 minutes")) %>%
    left_join(pickup_counts, by = "time_interval") %>%
    mutate(time_interval = as.POSIXct(time_interval, format = "%Y-%m-%d %H:%M:%S", tz = "UTC"))
  
  return(data_with_intervals)
}
```

```{r}
# Apply the function to the 1000-meter filtered data
train_filtered_1000m_data <- create_time_intervals(train_filtered_1000m_data, pickup_counts_1000m)

# Check the first few rows of the updated 1000m dataset
head(train_filtered_1000m_data)
```

```{r}
# Apply the function to the 2000-meter filtered data
train_filtered_2000m_data <- create_time_intervals(train_filtered_2000m_data, pickup_counts_2000m)

# Check the first few rows of the updated 2000m dataset
train_filtered_2000m_data
```

Visualization that shows top 10 pickup counts for both the 1000-meter and 2000-meter filtered data:

```{r}
plot_top_10_pickup_intervals <- function(data, title, color) {
  # Arrange the data to get the top 10 intervals with the highest pickup counts
  top_10_pickups <- data %>%
    arrange(desc(number_of_pickups)) %>%
    head(10)
  
  # Convert time_interval to a character with both date and time format
  top_10_pickups <- top_10_pickups %>%
    mutate(time_interval = format(time_interval, "%Y-%m-%d %H:%M"))
  
  # Plot the top 10 intervals
  ggplot(top_10_pickups, aes(x = time_interval, y = number_of_pickups)) +
    geom_bar(stat = "identity", fill = color, width = 0.6) + # Adjust width for spacing
    labs(title = title,
         x = "Time Interval",
         y = "Pickup Counts") +
    theme_minimal() +
    theme(
      axis.text.x = element_text(angle = 45, hjust = 1, size = 10), # Adjust text angle and size
      axis.text.y = element_text(size = 10),
      plot.margin = margin(t = 10, r = 10, b = 50, l = 10)          # Increase bottom margin
    ) +
    scale_x_discrete(labels = function(x) str_wrap(x, width = 10))  # Wrap the x-axis labels for better spacing
}
```

```{r}
# Plot for 1000-meter filtered data (lightblue color)
plot_top_10_pickup_intervals(pickup_counts_1000m, "Top 10 Pickup Intervals (1000m Radius)", "lightblue")
```
1000-meter Radius:

* The pickup counts in the top 10 intervals are around 150-175 pickups per interval.
* The busiest intervals span across different dates, showing a peak around July 15, 2014, with pickups happening consistently between 17:45 and 18:30 on several days.
* This indicates that Uber demand was high around these specific dates and times near the Empire State Building within a 1000-meter radius.

```{r}
# Plot for 2000-meter filtered data (lightgreen color)
plot_top_10_pickup_intervals(pickup_counts_2000m, "Top 10 Pickup Intervals (2000m Radius)", "lightgreen")
```
2000-meter Radius:

* The pickup counts in the top 10 intervals are significantly higher, around 500 pickups per interval.
* The busiest intervals are mostly concentrated on September 6, 2014, with intervals spanning between 17:15 and 23:30.
* This shows that the Uber demand beyond a 2000-meter radius from the Empire State Building was much higher on that specific date, indicating a spike in activity.

### Data Cleaning
#### Dealing with NAs 
Check for NA in the datasets:

```{r}
# Function to count NAs for each column in the dataset
count_nas_per_column <- function(data) {
  na_counts <- sapply(data, function(col) sum(is.na(col)))
  return(na_counts)
}
```

```{r}
# For 1000m data
nas_1000m <- count_nas_per_column(train_filtered_1000m_data)
print("NA counts for 1000m data:")
print(nas_1000m)
```

```{r}
# For 2000m data
nas_2000m <- count_nas_per_column(train_filtered_2000m_data)
print("NA counts for 2000m data:")
print(nas_2000m)

```

```{r NAs}
# Check for NA values in the 1000m data
na_count_1000m <- sum(is.na(train_filtered_1000m_data))
print(paste("Number of NA values in 1000m data:", na_count_1000m))

# Check for NA values in the 2000m data
na_count_2000m <- sum(is.na(train_filtered_2000m_data))
print(paste("Number of NA values in 2000m data:", na_count_2000m))
```
There is no NAs in both of the filltered data set!

#### Dealing with errors in the data
Check that there are no errors in timestamps (like invalid days, months, hours, minutes, or seconds):
```{r errors}
check_invalid_timestamps <- function(data) {
  # Extract the individual components from the timestamp and check for invalid values
  invalid_dates <- data %>%
    mutate(
      year = year(timestamp),
      month = month(timestamp),
      day = day(timestamp),
      hour = hour(timestamp),
      minute = minute(timestamp),
      second = second(timestamp)
    ) %>%
    filter(month > 12 | day > 31 | hour > 23 | minute > 59 | second > 59)
  
  # Print the results
  if (nrow(invalid_dates) > 0) {
    print("Invalid timestamps found:")
    print(invalid_dates)
  } else {
    print("All timestamps are valid.")
  }
}
```

```{r}
# Check for invalid timestamps in the 1000m data
cat("For 1000m data:\n")
check_invalid_timestamps(train_filtered_1000m_data)
```
```{r}
# Check for invalid timestamps in the 2000m data
cat("For 2000m data:\n")
check_invalid_timestamps(train_filtered_2000m_data)
```

Check if all the dates in the timestamp column are within the correct range (April 1, 2014, to September 9, 2014):
```{r}
# Function to check if timestamps are within a valid range
check_date_range <- function(data, start_date, end_date) {
  # Filter for invalid dates outside the specified range
  invalid_dates <- data %>%
    filter(timestamp < start_date | timestamp > end_date)
  
  # Check and print results
  if (nrow(invalid_dates) > 0) {
    cat("Invalid timestamps found:\n")
    print(invalid_dates)
  } else {
    cat("All timestamps are within the correct range.\n")
  }
}
```

```{r}
# Define the valid date range
start_date <- as.POSIXct("2014-04-01 00:00:00", tz = "UTC")
end_date <- as.POSIXct("2014-09-09 23:59:59", tz = "UTC")
```

```{r}
# Check for the 1000m data
cat("For 1000m data:\n")
check_date_range(train_filtered_1000m_data, start_date, end_date)
```

```{r}
# Check for the 2000m data
cat("For 2000m data:\n")
check_date_range(train_filtered_2000m_data, start_date, end_date)
```
Check for errors in Latitude (lat) & Longitude (lon):

1. Latitude values should range between -90 and 90 (since latitude represents how far north or south a location is from the equator).

2.Longitude values should range between -180 and 180 (since longitude represents how far east or west a location is from the prime meridian).

```{r}
# Function to check for errors in 'lat', 'lon', and 'base'
check_lat_lon <- function(data) {
  # Filter for invalid latitude, longitude, or missing base
  invalid_data <- data %>%
    filter(lat < -90 | lat > 90 | 
           lon < -180 | lon > 180)
  
  # Check and print results
  if (nrow(invalid_data) > 0) {
    cat("Invalid latitude or longitude:\n")
    print(invalid_data)
  } else {
    cat("All lat & lon values are valid.\n")
  }
}
```

```{r}
# Check for the 1000m data
cat("For 1000m data:\n")
check_lat_lon(train_filtered_1000m_data)
```

```{r}
# Check for the 2000m data
cat("For 2000m data:\n")
check_lat_lon(train_filtered_2000m_data)
```
Check for errors in Base code:
Function to list all unique bases:
```{r}
list_unique_bases <- function(data) {
  unique_bases <- unique(data$base)
  return(unique_bases)
}
```

```{r}
# List unique bases in the 1000m dataset
unique_bases_1000m <- list_unique_bases(train_filtered_1000m_data)
print("Unique base codes in the 1000m dataset:")
print(unique_bases_1000m)
```

```{r}
# List unique bases in the 2000m dataset
unique_bases_2000m <- list_unique_bases(train_filtered_2000m_data)
print("Unique base codes in the 2000m dataset:")
print(unique_bases_2000m)
```
```{r}
# Function to compare unique bases between two datasets
compare_unique_bases <- function(bases_1000m, bases_2000m) {
  
  # Check if both sets have the same items
  same_bases <- setequal(bases_1000m, bases_2000m)
  
  if (same_bases) {
    print("Both 1000m and 2000m datasets have the same unique bases.")
  } else {
    print("The 1000m and 2000m datasets have different unique bases.")
    
    # Identify the bases that are in 1000m but not in 2000m
    diff_1000m <- setdiff(bases_1000m, bases_2000m)
    if (length(diff_1000m) > 0) {
      print("Bases present in 1000m but not in 2000m:")
      print(diff_1000m)
    }
    
    # Identify the bases that are in 2000m but not in 1000m
    diff_2000m <- setdiff(bases_2000m, bases_1000m)
    if (length(diff_2000m) > 0) {
      print("Bases present in 2000m but not in 1000m:")
      print(diff_2000m)
    }
  }
}

# Run the function to compare unique bases
compare_unique_bases(unique_bases_1000m, unique_bases_2000m)
```
We used: https://www.nyc.gov/assets/tlc/downloads/pdf/trip_record_user_guide.pdf to find the Base Name:
B02598 - HINTER LLC 
B02617 - WEITER LLC
B02682 - SCHMECKEN LLC
B02764 - DANACH-NY,LLC
B02512 - UNTER LLC

* It appears that all the base codes in the dataset are properly identified and correspond to valid base names, suggesting that there are no meaningless or invalid codes. Each base code is linked to a registered entity.

* The base names seem to be associated with different companies, but the names themselves may not directly provide information about the service type or location.

#### Dealing with outliers
```{r outliers}
# not sure if needed

```

```{r}
train_filtered_1000m_data
```

## Part B: Exploratory Data Analysis (EDA)
### 1. Data Preparation
#### Date base data: Adding Columns
We added in the Data Rearrangement part:
1. number_of_pickups: this is how many pickups were is this specific 15-min interval (the label).

2. dist (for the 1000m data): this is the distance from each pickup to the Empire State Building

We will also add:
1.is_weekend: Extract whether the pickup occurred on a weekday or weekend. This is important because pickup patterns often differ on weekdays versus weekends.

2. time_of_day: Group pickup times into time categories such as morning, afternoon, evening, and night to capture demand patterns.

3. Dummy of the Bases: onvert the categorical base variable into dummy (one-hot encoded) variables.

Add is_weekend Feature
```{r adding_is_weekend}
# Function to add 'is_weekend' feature only
add_is_weekend <- function(data) {
  data %>%
    mutate(
      # Check if it's a weekend (Saturday or Sunday)
      is_weekend = ifelse(wday(timestamp, label = TRUE) %in% c("Sat", "Sun"), 1, 0)
    )
}
```

```{r}
# Apply the function to the 1000m dataset
train_filtered_1000m_data <- add_is_weekend(train_filtered_1000m_data)
head(train_filtered_1000m_data)
```

```{r}
# Apply the function to the 2000m dataset
train_filtered_2000m_data <- add_is_weekend(train_filtered_2000m_data)
head(train_filtered_2000m_data)
```

Add time_of_day Feature (in both categorical form & dummy form):

Step 1: Adding time_of_day as a Categorical Feature
* This feature assigns a number to each time period of the day (Night = 1, Morning = 2, Afternoon = 3, Evening = 4). This approach is suitable for tree-based models like decision trees, random forests, or gradient boosting machines (GBM), as these models handle categorical variables natively and can split data based on these categories without assuming any linear relationship.

```{r}
# Function to add 'time_of_day' as a categorical feature
add_categorical_time_of_day <- function(data) {
  data %>%
    mutate(
      hour = hour(timestamp),
      time_of_day = case_when(
        hour >= 0 & hour < 6  ~ 1,  # Night
        hour >= 6 & hour < 12 ~ 2,  # Morning
        hour >= 12 & hour < 18 ~ 3,  # Afternoon
        hour >= 18 & hour <= 23 ~ 4   # Evening
      )
    )
}
```

```{r}
# Apply the function to the 1000m dataset
train_filtered_1000m_data <- add_categorical_time_of_day(train_filtered_1000m_data)
head(train_filtered_1000m_data)
```

```{r}
# Apply the function to the 2000m dataset
train_filtered_2000m_data <- add_categorical_time_of_day(train_filtered_2000m_data)
head(train_filtered_2000m_data)
```

Step 2: Adding time_of_day as Dummy Variables (One-Hot Encoding)
* In this approach, we create separate columns for each time period (Night, Morning, Afternoon, Evening). This is suitable for linear models (e.g., linear regression, logistic regression), which expect numeric inputs and might incorrectly interpret a categorical variable like 1, 2, 3, 4 as implying some order or linear relationship. With dummy variables, each time period is independent, preventing any false assumptions about relationships between time periods.

Night: 00:00–05:59
Morning: 06:00–11:59
Afternoon: 12:00–17:59
Evening: 18:00–23:59

Here’s the code for adding the time_of_day feature using one-hot encoding, broken down into separate dummy features (Night, Morning, Afternoon, Evening):

```{r adding_time_of_day}
# Function to add 'time_of_day' as one-hot encoded dummy variables
add_dummy_time_of_day <- function(data) {
  data %>%
    mutate(
      hour = hour(timestamp),
      # Create dummy variables for time of day
      is_night = ifelse(hour >= 0 & hour < 6, 1, 0),
      is_morning = ifelse(hour >= 6 & hour < 12, 1, 0),
      is_afternoon = ifelse(hour >= 12 & hour < 18, 1, 0),
      is_evening = ifelse(hour >= 18 & hour <= 23, 1, 0)
    ) %>%
    select(-is_night)  # Remove one category:'is_night' to avoid dummy variable trap
}
```

```{r}
# Apply the function to the 1000m dataset
train_filtered_1000m_data <- add_dummy_time_of_day(train_filtered_1000m_data)
head(train_filtered_1000m_data)
```

```{r}
# Apply the function to the 2000m dataset
train_filtered_2000m_data <- add_dummy_time_of_day(train_filtered_2000m_data)
head(train_filtered_2000m_data)
```
Add the dummy of the bases:
Similar to the time_of_day feature, we need to convert the categorical base variable into dummy (one-hot encoded) variables when working with linear models. This prevents the model from assuming any ordinal relationship between the base codes. Since we have five base codes (B02598, B02617, B02682, B02764, B02512), we will create dummy variables for each, but we will remove one of the dummies to avoid the dummy variable trap (when all dummy features are 0, the removed category is 1). This is important in models like linear regression to prevent multicollinearity.

```{r}
# Function to add dummy variables for base codes and remove one dummy to avoid the dummy trap
add_dummy_bases <- function(data) {
  data %>%
    mutate(
      is_B02598 = ifelse(base == "B02598", 1, 0),
      is_B02617 = ifelse(base == "B02617", 1, 0),
      is_B02682 = ifelse(base == "B02682", 1, 0),
      is_B02764 = ifelse(base == "B02764", 1, 0)
      # B02512 will be the removed base (if all are 0, it's B02512)
    )
}
```

```{r}
# Apply the function to the 1000m dataset
train_filtered_1000m_data <- add_dummy_bases(train_filtered_1000m_data)
head(train_filtered_1000m_data)
```

```{r}
# Apply the function to the 2000m dataset
train_filtered_2000m_data <- add_dummy_bases(train_filtered_2000m_data)
head(train_filtered_2000m_data)
```

Explanation for Model Usage:
* Tree-Based Models (like decision trees, random forests, etc.) can directly handle the categorical base codes and the time_of_day feature in its original categorical form. These models don’t require dummy variables and can split the data based on the base or time of day without assuming any linear relationship.

* Linear Models (like linear regression, logistic regression, etc.) require dummy variables for categorical features like base and time_of_day to avoid false assumptions about relationships between different categories. Dummy variables prevent the model from interpreting categories like B02598 and B02617 as having a linear relationship, which would distort the model.

* Dummy variable trap: When using one-hot encoding, it's common to drop one category (here, is_night & B02512) because its value can be inferred when all other dummy variables are 0. This avoids redundancy and multicollinearity.

By adding both a categorical feature and dummy variables for time_of_day & base, later on we will switch between the two versions depending on the model we will use:
* Categorical feature for tree-based models.
* Dummy variables for linear models.

#### Data from the Web: External Data Integration

Here is a CSV of the history weather bulk for Empire State Building (40.75,-73.99) from January 01, 2014 to December 31, 2014
```{r}
# Load the CSV file
weather_data <- read_csv("empire-state-weather-2014.csv")

# Convert the 'dt_iso' column to POSIXct format and create the new 'timestamp' column
weather_data <- weather_data %>%
  mutate(timestamp = as.POSIXct(dt_iso, format = "%Y-%m-%d %H:%M:%S", tz = "UTC"))

# Create 'is_raining_last_hour' and 'is_snowing_last_hour' features
weather_data <- weather_data %>%
  mutate(is_raining_last_hour = ifelse(!is.na(rain_1h) & rain_1h > 0, 1, 0),
         is_snowing_last_hour = ifelse(!is.na(snow_1h) & snow_1h > 0, 1, 0))

# Check the first few rows to see the new columns
head(weather_data)
```

Code to Check Minutes and Seconds in Timestamps:
```{r}
# Filter rows where the minute or second is not 0
non_zero_minute_or_second <- weather_data %>%
  filter(minute(timestamp) != 0 | second(timestamp) != 0)

# Check if there are any rows with non-zero minute or second values
if (nrow(non_zero_minute_or_second) > 0) {
  print("Rows with non-zero minutes or seconds found:")
  print(non_zero_minute_or_second)
} else {
  print("All timestamps have 0 for both minutes and seconds.")
}
```

We have confirmed that the weather data is recorded at exact hourly intervals, meaning the data is consistent without variations in the minutes or seconds. To merge this data with our Uber pickup data, we need to ensure our timestamps align correctly. Since the weather information is relevant for each hour as recorded, we will always round the Uber pickup timestamps down to the current hour to maintain accuracy and ensure a seamless merge.

This code combines weather data (temperature, humidity, wind speed, and snow indication) with the Uber pickup data (1000m and 2000m datasets) using the timestamp as the key. Rather than adjusting the timestamps based on the minute, we round all timestamps to the start of the current hour since the weather data is available in one-hour increments. This approach ensures that the weather conditions are consistently matched to the correct hour.

We are using a function called rounded_timestamp that adds a new feature, rounded_timestamp, to the Uber pickup datasets. This feature rounds the timestamp down to the start of the current hour to align the weather data with the Uber pickup data based on the adjusted timestamps.
```{r}
# Function to round the timestamp to the current hour
rounded_timestamp <- function(df) {
  df %>%
    mutate(
      rounded_timestamp = as.POSIXct(floor_date(timestamp, "hour"), format="%Y-%m-%d %H:%M:%S", tz = "UTC")
    )
}
```

```{r}
# Round timestamps for 1000m data
train_filtered_1000m_data <- rounded_timestamp(train_filtered_1000m_data)
train_filtered_1000m_data
```

```{r}
# Round timestamps for 2000m data
train_filtered_2000m_data <- rounded_timestamp(train_filtered_1000m_data)
train_filtered_2000m_data
```


```{r}
# Select only the required columns from the weather data
weather_data_selected <- weather_data %>%
  select(timestamp, temp, humidity, wind_speed, is_snowing_last_hour)

# Merge the weather data with the 1000m dataset using 'rounded_timestamp'
new_features_1000m_data <- train_filtered_1000m_data %>%
  left_join(weather_data_selected, by = c("rounded_timestamp" = "timestamp"))

# Check the first few rows of the new dataset to verify the merge
head(new_features_1000m_data)
```

```{r}
# Merge the weather data with the 2000m dataset using 'rounded_timestamp'
new_features_2000m_data <- train_filtered_2000m_data %>%
  left_join(weather_data_selected, by = c("rounded_timestamp" = "timestamp"))

# Check the first few rows of the new dataset to verify the merge
head(new_features_2000m_data)
```

### 2. Exploratory Analysis
#### Descriptive Statistics and Visualization
```{r descriptive_statistics}
# קוד לסטטיסטיקות תיאוריות ויצירת גרפים
```

## Part C: Forecast for the Future

### Model 1 - Full Data
```{r model_1_full_data}
# קוד לאימון המודל הראשון באמצעות הנתונים המלאים
```

### Model 2 - Filtered Data
```{r model_2_filtered_data}
# קוד לאימון המודל השני באמצעות הנתונים המסוננים (מחוץ לרדיוס 2000 מטר)
```

## Conclusion
```{r conclusion}
# סיכום הממצאים והתובנות מהמודלים
```