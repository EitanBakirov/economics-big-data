---
title: "big_data_final_task"
author: "Ariel&Eitan&Yuval"
date: "2024-10-05"
output: html_document
---

# Project: Predict Uber Demand in New York

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Load necessary libraries
```{r necessary_libraries}
library(tidyr)
library(dplyr)
library(geosphere) 
# library(leaflet)
library(ggplot2)
library(lubridate)
library(readr)
library(stringr)
```

## Load the CSV files
```{r load_data}
train_raw_data <- read.csv("train_raw_data.csv")
train_2000m_data <- read.csv("train_raw_data_dists_more_then_2000.csv")
test_data <- read.csv("test_set.csv")
```

### train_raw_data.csv
train_raw_data.csv: Contains all Uber pickups in NYC between April 1 and September 9, 2014, without filtering by distance.

```{r raw_data_look}
# Show the first 5 rows of each train_raw_data
head(train_raw_data, 5)

# Summary statistics for the full dataset
summary(train_raw_data)
```
* Latitude (lat) ranges from 39.66 to 42.12, and Longitude (lon) ranges from -74.93 to -72.07, indicating the pickups occurred within the New York area.

* The median and mean values for both latitude (around 40.74) and longitude (around -73.97) show that most pickups took place near the central part of New York, close to the Empire State Building.

### train_raw_data_dists_more_then_2000.csv
train_raw_data_dists_more_then_2000.csv: Contains only Uber pickups outside a 2000-meter radius from the Empire State Building.

```{r filtered_data_look}
# Show the first 5 rows of each train_raw_data
head(train_2000m_data, 5)

# Summary statistics for the filtered dataset
summary(train_2000m_data)
```

* Similar latitude and longitude ranges, but these pickups occurred outside a 2000-meter radius from the Empire State Building.

* The dist column represents the distance from the Empire State Building, ranging from 2000 meters to 220,970 meters (about 221 km). The median distance is 4083 meters.

#### Features Explanation:
Datasets train_raw_data.csv and train_raw_data_dists_more_then_2000.csv features:

1. datetime: This column represents the date and time when the Uber pickup occurred.

2. lat: This column contains the latitude of the pickup location, indicating the geographical position (north-south axis) of the ride.

3. lon: This column contains the longitude of the pickup location, representing the geographical position (east-west axis) of the ride.

4. Base:  This column includes a code representing the TLC (Taxi and Limousine Commission) base company code affiliated with the Uber pickup. This code identifies the licensed dispatching base that managed the Uber trip. Every ride must be affiliated with a TLC-licensed base, which is responsible for dispatching the trip.(License Number).

5. dist (only for train_raw_data_dists_more_then_2000.csv): This column represents the distance (in meters) between the pickup location and the Empire State Building. All values in this dataset are greater than 2000 meters, as the dataset is filtered to include only pickups beyond this radius.

### test_set.csv
```{r}
test_data
```
We can observe that the test file contains time intervals in 15-minute increments, currently stored in character format. We need to convert these time intervals to POSIXct format for further analysis. The objective is then to use this data to predict the number of pickups using a trained model.

```{r}
test_fixed_data <- test_data %>%
  mutate(time_interval = as.POSIXct(time_interval, format="%Y-%m-%dT%H:%M:%SZ", tz = "UTC"))

test_fixed_data
```
Note: In order to predict the number_of_pickups using a ML model, our test data should have the same features that were used to train the model.So, it is crucial to ensure that all the transformations, feature engineering steps, creating new features and any external data applied during the training process are also applied to the test data. Ensuring consistency in features allows the model to process the test data accurately and make reliable predictions based on the learned relationships during training.

Looking at the map:
```{r interactive_map}
# # Define latitude, longitude, and radius
# lat <- 40.7484 # Empire State Building coordinates
# lon <- -73.985 # Empire State Building coordinates
# 
# small_radius <- 1000  # Radius in meters
# big_radius <- 2000  # Radius in meters
# 
# # Create the leaflet map
# leaflet() %>%
#   addTiles() %>%  # Add default OpenStreetMap tiles
#   addMarkers(lng = lon, lat = lat, popup = "Empire State Building") %>%  # 
#   addCircles(lng = lon, lat = lat, radius = small_radius, color = "blue", fillOpacity = 0.2) %>% # 
#   addCircles(lng = lon, lat = lat, radius = big_radius, color = "red", fillOpacity = 0.1)  #
```

#### Is 2000m is in train_raw?
```{r filtered_data_look}

# str(train_filtered_2000m_data)
# str(train_raw_data)
# 
# # Ensure both data frames have the same columns before comparison
# common_columns <- intersect(names(train_filtered_2000m_data), names(train_raw_data))
# 
# # Select only the common columns for comparison
# train_filtered_2000m_data_common <- train_filtered_2000m_data %>% select(all_of(common_columns))
# train_raw_data_common <- train_raw_data %>% select(all_of(common_columns))
# 
# # Now use setdiff to find rows in train_filtered_2000m_data not in train_raw_data
# missing_rows <- setdiff(train_filtered_2000m_data_common, train_raw_data_common)
# 
# # Check if any rows are missing
# if (nrow(missing_rows) == 0) {
#   print("All rows in train_filtered_2000m_data are present in train_raw_data.")
# } else {
#   print("Some rows in train_filtered_2000m_data are missing from train_raw_data.")
#   print(missing_rows)
# }

```
So, 2,623  rows in 2000m are unique - (not in train_raw_data)

#### Is 2000 above from train_data in 2000m?
```{r filtered_data_look}

# # Create a new dataframe with rows where dist is greater than or equal to 2000
# check_train_raw_above_2000 <- train_raw_data %>%
#   filter(dist >= 2000)
# 
# # Ensure both data frames have the same columns before comparison
# common_columns <- intersect(names(check_train_raw_above_2000), names(train_filtered_2000m_data))
# 
# # Select only common columns for comparison
# check_train_raw_above_2000_common <- check_train_raw_above_2000 %>% select(all_of(common_columns))
# train_filtered_2000m_data_common <- train_filtered_2000m_data %>% select(all_of(common_columns))
# 
# # Now use setdiff
# missing_rows <- setdiff(check_train_raw_above_2000_common, train_filtered_2000m_data_common)
# 
# # Check if any rows are missing
# if (nrow(missing_rows) == 0) {
#   print("All rows in check_train_raw_above_2000 are present in train_filtered_2000m_data.")
# } else {
#   print("Some rows in check_train_raw_above_2000 are missing from train_filtered_2000m_data.")
#   print(missing_rows)
# }
```

## Part A: Data Rearrangement

### Filtering Data
Only relevant for the train_raw_data.csv data:
```{r}
# Define Empire State Building coordinates
empire_state_coords <- c(-73.985, 40.7484)  # Longitude, Latitude

# Calculate the distance from each pickup to the Empire State Building
train_raw_data <- train_raw_data %>%
  mutate(dist = distHaversine(cbind(lon, lat), empire_state_coords))
```

We would like to see how our data is distributed:
```{r}
# Add a new column 'distance_category' to classify the distance
train_raw_data <- train_raw_data %>%
  mutate(distance_category = ifelse(dist <= 1000, "<= 1000 meters", "> 1000 meters"))

ggplot(train_raw_data, aes(x = distance_category)) +
  geom_bar(fill = c("lightblue", "lightgreen")) +
  labs(title = "Count of Uber Pickups within and beyond 1000 meters",
       x = "Distance Category",
       y = "Count of Pickups") +
  theme_minimal()
```

We can see that there are many Uber pickups occurred beyond the 1000-meter radius that need to be filtered.

```{r filtering_data}
# Filter data to include only pickups within a 1000-meter radius and remove the 'distance_category' column
train_filtered_1000m_data <- train_raw_data %>%
  filter(dist <= 1000) %>%
  select(-distance_category)  # Remove the 'distance_category' column if it exists
```

Making sure the filter is done
```{r}
# Check the maximum distance in the filtered dataset
max_distance <- max(train_filtered_1000m_data$dist)

# Print the maximum distance
print(max_distance)
```
Let's check how many rows in this dataset we are left with:
```{r}
rows_1000m <- nrow(train_filtered_1000m_data)
print(paste("Number of rows in the 1000m dataset:", rows_1000m))

rows_2000m <- nrow(train_2000m_data)
print(paste("Number of rows in the 2000m dataset:", rows_2000m))
```

```{r}
train_filtered_1000m_data
```

```{r}
train_2000m_data
```

### Filtering by Time
We need to make sure the timestamp column is formatted correctly.
* The timestamp format (for example: 2014-04-01T00:02:00Z) includes a "T" between the date and time and ends with a "Z", which indicates that the time is in UTC (Coordinated Universal Time).
We need to adjust the format argument to handle this format. Additionally, we can specify the time zone using tz = "UTC" to make sure the conversion handles the UTC time correctly.
```{r}
# Ensure the 'timestamp' column is in POSIXct format, handling the 'T' and 'Z' characters
train_filtered_1000m_data <- train_filtered_1000m_data %>%
  mutate(timestamp = as.POSIXct(timestamp, format="%Y-%m-%dT%H:%M:%SZ", tz = "UTC"))

train_filtered_2000m_data <- train_2000m_data %>%
  mutate(timestamp = as.POSIXct(timestamp, format="%Y-%m-%dT%H:%M:%SZ", tz = "UTC"))
```

Visualize the distribution of Uber pickups by hour to check if there are any outside 17:00-00:00:
```{r}
plot_pickup_distribution <- function(data, title, color) {
  ggplot(data, aes(x = hour(timestamp))) +
    geom_bar(fill = color) +
    labs(title = title,
         x = "Hour of Day",
         y = "Count of Pickups") +
    scale_x_continuous(breaks = 0:23) +  # Set x-axis for hours (0 to 23)
    theme_minimal() +
    theme(axis.text.x = element_text(angle = 45, hjust = 1))  # Rotate x-axis labels for clarity
}
```
Lets say that each dataset will get a color: lightblue for the 1000m data and lightgreen for the 2000m data

```{r}
# For 1000-meter filtered data
plot_pickup_distribution(train_filtered_1000m_data, "Distribution of Uber Pickups by Hour (1000m Data)", "lightblue")
```

```{r}
# For 2000-meter filtered data
plot_pickup_distribution(train_filtered_2000m_data, "Distribution of Uber Pickups by Hour (2000m Data)", "lightgreen")
```
As seen in the two graphs, there are significant amounts of data outside the desired range of 17:00 to 00:00 for both datasets (1000m and 2000m data). Therefore, to focus on the data for pickups that occurred between 17:00 and 00:00, we will apply a filter to exclude the hours outside this range.

Filter data to include only pickups between 17:00 and 00:00:
```{r}
# For 1000-meter filtered data
train_filtered_1000m_data <- train_filtered_1000m_data %>%
  filter(hour(timestamp) >= 17 | hour(timestamp) == 0)

# For 2000-meter filtered data
train_filtered_2000m_data <- train_filtered_2000m_data %>%
  filter(hour(timestamp) >= 17 | hour(timestamp) == 0)
```

Check the Results:
```{r}
# Create a data frame with all hours from 0 to 23
hours <- data.frame(hour = 0:23)
```

```{r}
plot_hourly_pickup_distribution <- function(data, hours_df, title, color) {
  # Extract the hour and count the number of pickups per hour
  hourly_data <- data %>%
    mutate(hour = hour(timestamp)) %>%
    count(hour)
  
  # Merge the hourly counts with the 'hours' data frame to include missing hours as 0
  hourly_data_complete <- merge(hours_df, hourly_data, by = "hour", all.x = TRUE)
  hourly_data_complete[is.na(hourly_data_complete)] <- 0  # Replace NAs with 0
  
  # Plot the hourly data
  ggplot(hourly_data_complete, aes(x = hour, y = n)) +
    geom_bar(stat = "identity", fill = color) +
    labs(title = title,
         x = "Hour of Day",
         y = "Count of Pickups") +
    scale_x_continuous(breaks = 0:23) +  # Show all hours from 0 to 23 on x-axis
    theme_minimal()
}
```

```{r}
# Plot for 1000-meter filtered data (lightblue color)
plot_hourly_pickup_distribution(train_filtered_1000m_data, hours, 
                                "Filtered Uber Pickups by Hour (1000m Data, Expected: 17:00 to 00:00)", 
                                "lightblue")
```
```{r}
# Plot for 2000-meter filtered data (lightgreen color)
plot_hourly_pickup_distribution(train_filtered_2000m_data, hours, 
                                "Filtered Uber Pickups by Hour (2000m Data, Expected: 17:00 to 00:00)", 
                                "lightgreen")
```
We are left with only rows with the desired time range!

### Creating 15-Minute Intervals
We're grouping the `timestamp` data into 15-minute intervals to make it easier to spot patterns and trends in Uber pickups. This lets us count how many pickups happen in each time block, helping us better understand demand and predict future activity.

This action is critical because our test data is already divided into 15-minute intervals, so we need to apply the same approach to our training data to ensure consistency. By creating the number_of_pickups feature (the label), we align both datasets in terms of granularity, enabling the model to learn from the training data and make accurate predictions on the test data.

The function takes a dataset as input, creates the 15-minute time intervals, and calculates the pickup counts per interval:

```{r}
create_pickup_counts <- function(data) {
  pickup_counts <- data %>%
    mutate(time_interval = floor_date(timestamp, "15 minutes")) %>%
    group_by(time_interval) %>%
    summarise(number_of_pickups = n())
  
  return(pickup_counts)
}
```

```{r}
# Use the function for 1000m data
pickup_counts_1000m <- create_pickup_counts(train_filtered_1000m_data)
head(pickup_counts_1000m)
```

```{r}
# Use the function for 2000m data
pickup_counts_2000m <- create_pickup_counts(train_filtered_2000m_data)
head(pickup_counts_2000m)
```

```{r time_intervals}
create_time_intervals <- function(data, pickup_counts) {
  # Join the pickup counts back to the original data to include all columns
  data_with_intervals <- data %>%
    mutate(time_interval = floor_date(timestamp, "15 minutes")) %>%
    left_join(pickup_counts, by = "time_interval") %>%
    mutate(time_interval = as.POSIXct(time_interval, format = "%Y-%m-%d %H:%M:%S", tz = "UTC"))
  
  return(data_with_intervals)
}
```

```{r}
# Apply the function to the 1000-meter filtered data
train_filtered_1000m_data <- create_time_intervals(train_filtered_1000m_data, pickup_counts_1000m)

# Check the first few rows of the updated 1000m dataset
head(train_filtered_1000m_data)
```

```{r}
# Apply the function to the 2000-meter filtered data
train_filtered_2000m_data <- create_time_intervals(train_filtered_2000m_data, pickup_counts_2000m)

# Check the first few rows of the updated 2000m dataset
train_filtered_2000m_data
```

Visualization that shows top 10 pickup counts for both the 1000-meter and 2000-meter filtered data:

```{r}
plot_top_10_pickup_intervals <- function(data, title, color) {
  # Arrange the data to get the top 10 intervals with the highest pickup counts
  top_10_pickups <- data %>%
    arrange(desc(number_of_pickups)) %>%
    head(10)
  
  # Convert time_interval to a character with both date and time format
  top_10_pickups <- top_10_pickups %>%
    mutate(time_interval = format(time_interval, "%Y-%m-%d %H:%M"))
  
  # Plot the top 10 intervals
  ggplot(top_10_pickups, aes(x = time_interval, y = number_of_pickups)) +
    geom_bar(stat = "identity", fill = color, width = 0.6) + # Adjust width for spacing
    labs(title = title,
         x = "Time Interval",
         y = "Pickup Counts") +
    theme_minimal() +
    theme(
      axis.text.x = element_text(angle = 45, hjust = 1, size = 10), # Adjust text angle and size
      axis.text.y = element_text(size = 10),
      plot.margin = margin(t = 10, r = 10, b = 50, l = 10)          # Increase bottom margin
    ) +
    scale_x_discrete(labels = function(x) str_wrap(x, width = 10))  # Wrap the x-axis labels for better spacing
}
```

```{r}
# Plot for 1000-meter filtered data (lightblue color)
plot_top_10_pickup_intervals(pickup_counts_1000m, "Top 10 Pickup Intervals (1000m Radius)", "lightblue")
```
1000-meter Radius:

* The pickup counts in the top 10 intervals are around 150-175 pickups per interval.
* The busiest intervals span across different dates, showing a peak around July 15, 2014, with pickups happening consistently between 17:45 and 18:30 on several days.
* This indicates that Uber demand was high around these specific dates and times near the Empire State Building within a 1000-meter radius.

```{r}
# Plot for 2000-meter filtered data (lightgreen color)
plot_top_10_pickup_intervals(pickup_counts_2000m, "Top 10 Pickup Intervals (2000m Radius)", "lightgreen")
```
2000-meter Radius:

* The pickup counts in the top 10 intervals are significantly higher, around 500 pickups per interval.
* The busiest intervals are mostly concentrated on September 6, 2014, with intervals spanning between 17:15 and 23:30.
* This shows that the Uber demand beyond a 2000-meter radius from the Empire State Building was much higher on that specific date, indicating a spike in activity.

### Data Cleaning
#### Dealing with NAs 
Check for NA in the datasets:

```{r}
# Function to count NAs for each column in the dataset
count_nas_per_column <- function(data) {
  na_counts <- sapply(data, function(col) sum(is.na(col)))
  return(na_counts)
}
```

```{r}
# For 1000m data
nas_1000m <- count_nas_per_column(train_filtered_1000m_data)
print("NA counts for 1000m data:")
print(nas_1000m)
```

```{r}
# For 2000m data
nas_2000m <- count_nas_per_column(train_filtered_2000m_data)
print("NA counts for 2000m data:")
print(nas_2000m)

```

```{r NAs}
# Check for NA values in the 1000m data
na_count_1000m <- sum(is.na(train_filtered_1000m_data))
print(paste("Number of NA values in 1000m data:", na_count_1000m))

# Check for NA values in the 2000m data
na_count_2000m <- sum(is.na(train_filtered_2000m_data))
print(paste("Number of NA values in 2000m data:", na_count_2000m))
```
There is no NAs in both of the filtered data sets!

#### Dealing with errors in the data
Check that there are no errors in timestamps (like invalid days, months, hours, minutes, or seconds):
```{r errors}
check_invalid_timestamps <- function(data) {
  # Extract the individual components from the timestamp and check for invalid values
  invalid_dates <- data %>%
    mutate(
      year = year(timestamp),
      month = month(timestamp),
      day = day(timestamp),
      hour = hour(timestamp),
      minute = minute(timestamp),
      second = second(timestamp)
    ) %>%
    filter(month > 12 | day > 31 | hour > 23 | minute > 59 | second > 59)
  
  # Print the results
  if (nrow(invalid_dates) > 0) {
    print("Invalid timestamps found:")
    print(invalid_dates)
  } else {
    print("All timestamps are valid.")
  }
}
```

```{r}
# Check for invalid timestamps in the 1000m data
cat("For 1000m data:\n")
check_invalid_timestamps(train_filtered_1000m_data)
```
```{r}
# Check for invalid timestamps in the 2000m data
cat("For 2000m data:\n")
check_invalid_timestamps(train_filtered_2000m_data)
```

Check if all the dates in the timestamp column are within the correct range (April 1, 2014, to September 9, 2014):
```{r}
# Function to check if timestamps are within a valid range
check_date_range <- function(data, start_date, end_date) {
  # Filter for invalid dates outside the specified range
  invalid_dates <- data %>%
    filter(timestamp < start_date | timestamp > end_date)
  
  # Check and print results
  if (nrow(invalid_dates) > 0) {
    cat("Invalid timestamps found:\n")
    print(invalid_dates)
  } else {
    cat("All timestamps are within the correct range.\n")
  }
}
```

```{r}
# Define the valid date range
start_date <- as.POSIXct("2014-04-01 00:00:00", tz = "UTC")
end_date <- as.POSIXct("2014-09-09 23:59:59", tz = "UTC")
```

```{r}
# Check for the 1000m data
cat("For 1000m data:\n")
check_date_range(train_filtered_1000m_data, start_date, end_date)
```

```{r}
# Check for the 2000m data
cat("For 2000m data:\n")
check_date_range(train_filtered_2000m_data, start_date, end_date)
```
Check for errors in Latitude (lat) & Longitude (lon):

1. Latitude values should range between -90 and 90 (since latitude represents how far north or south a location is from the equator).

2.Longitude values should range between -180 and 180 (since longitude represents how far east or west a location is from the prime meridian).

```{r}
# Function to check for errors in 'lat', 'lon', and 'base'
check_lat_lon <- function(data) {
  # Filter for invalid latitude, longitude, or missing base
  invalid_data <- data %>%
    filter(lat < -90 | lat > 90 | 
           lon < -180 | lon > 180)
  
  # Check and print results
  if (nrow(invalid_data) > 0) {
    cat("Invalid latitude or longitude:\n")
    print(invalid_data)
  } else {
    cat("All lat & lon values are valid.\n")
  }
}
```

```{r}
# Check for the 1000m data
cat("For 1000m data:\n")
check_lat_lon(train_filtered_1000m_data)
```

```{r}
# Check for the 2000m data
cat("For 2000m data:\n")
check_lat_lon(train_filtered_2000m_data)
```
Check for errors in Base code:
Function to list all unique bases:
```{r}
list_unique_bases <- function(data) {
  unique_bases <- unique(data$base)
  return(unique_bases)
}
```

```{r}
# List unique bases in the 1000m dataset
unique_bases_1000m <- list_unique_bases(train_filtered_1000m_data)
print("Unique base codes in the 1000m dataset:")
print(unique_bases_1000m)
```

```{r}
# List unique bases in the 2000m dataset
unique_bases_2000m <- list_unique_bases(train_filtered_2000m_data)
print("Unique base codes in the 2000m dataset:")
print(unique_bases_2000m)
```

```{r}
# Function to compare unique bases between two datasets
compare_unique_bases <- function(bases_1000m, bases_2000m) {
  
  # Check if both sets have the same items
  same_bases <- setequal(bases_1000m, bases_2000m)
  
  if (same_bases) {
    print("Both 1000m and 2000m datasets have the same unique bases.")
  } else {
    print("The 1000m and 2000m datasets have different unique bases.")
    
    # Identify the bases that are in 1000m but not in 2000m
    diff_1000m <- setdiff(bases_1000m, bases_2000m)
    if (length(diff_1000m) > 0) {
      print("Bases present in 1000m but not in 2000m:")
      print(diff_1000m)
    }
    
    # Identify the bases that are in 2000m but not in 1000m
    diff_2000m <- setdiff(bases_2000m, bases_1000m)
    if (length(diff_2000m) > 0) {
      print("Bases present in 2000m but not in 1000m:")
      print(diff_2000m)
    }
  }
}

# Run the function to compare unique bases
compare_unique_bases(unique_bases_1000m, unique_bases_2000m)
```
We used: https://www.nyc.gov/assets/tlc/downloads/pdf/trip_record_user_guide.pdf to find the Base Name:
B02598 - HINTER LLC 
B02617 - WEITER LLC
B02682 - SCHMECKEN LLC
B02764 - DANACH-NY,LLC
B02512 - UNTER LLC

* It appears that all the base codes in the dataset are properly identified and correspond to valid base names, suggesting that there are no meaningless or invalid codes. Each base code is linked to a registered entity.

* The base names seem to be associated with different companies, but the names themselves may not directly provide information about the service type or location.

Lets look at the bases in everyday:
```{r}
# Function to count and plot the number of rows for each base, with labels on top of each bar
plot_base_counts <- function(data, title, fill_color) {
  # Count the number of rows for each base
  base_counts <- data %>%
    group_by(base) %>%
    summarise(num_rows = n()) %>%
    arrange(desc(num_rows))
  
  # Create the bar plot
  ggplot(base_counts, aes(x = reorder(base, -num_rows), y = num_rows)) +
    geom_bar(stat = "identity", fill = fill_color) +
    geom_text(aes(label = scales::comma(num_rows)), 
              vjust = -0.5, size = 3.5) +  # Add labels on top of the bars
    labs(title = title,
         x = "Base",
         y = "Number of Rows") +
    theme_minimal() +
    theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
    scale_y_continuous(labels = scales::comma)  # Add comma format to avoid scientific notation
}
```

```{r}
# Apply the function to the 1000m dataset
plot_base_counts(train_filtered_1000m_data, "Number of Rows per Base (1000m Data)", "lightblue")

```

```{r}
# Apply the function to the 2000m dataset
plot_base_counts(train_filtered_2000m_data, "Number of Rows per Base (2000m Data)", "lightgreen")
```
```{r}
# Function to plot timeline of Uber pickups per day for each base
plot_base_timeline <- function(data, title) {
  # Extract the date from the timestamp and group by base and date
  base_daily_counts <- data %>%
    mutate(date = as.Date(timestamp)) %>%
    group_by(base, date) %>%
    summarise(count_pickups_per_day = n()) %>%
    ungroup()

  # Plot the data
  ggplot(base_daily_counts, aes(x = date, y = count_pickups_per_day, color = base, group = base)) +
    geom_line(size = 1) +
    labs(title = title,
         x = "Date",
         y = "Number of Pickups",
         color = "Base") +
    theme_minimal() +
    theme(axis.text.x = element_text(angle = 45, hjust = 1))
}
```

```{r}
# Apply the function to your dataset
plot_base_timeline(train_filtered_1000m_data, "Uber Pickups per Day by Base (1000m Data)")
```

```{r}
# Apply the function to your dataset
plot_base_timeline(train_filtered_2000m_data, "Uber Pickups per Day by Base (2000m Data)")
```

```{r}
# Function to plot days with zero pickups for each base
plot_zero_pickups <- function(data, title) {
  # Extract the date from the timestamp and group by base and date
  base_daily_counts <- data %>%
    mutate(date = as.Date(timestamp)) %>%
    group_by(base, date) %>%
    summarise(count_pickups_per_day = n(), .groups = 'drop') 
  
  # Create a complete sequence of dates for each base to account for missing days
  full_date_range <- base_daily_counts %>%
    group_by(base) %>%
    tidyr::complete(date = seq(min(date), max(date), by = "day"),
                    fill = list(count_pickups_per_day = 0)) %>%
    ungroup()
  
  # Filter for rows where the number of pickups is zero
  zero_pickups <- full_date_range %>%
    filter(count_pickups_per_day == 0)
  
  # Plot the data
  ggplot(zero_pickups, aes(x = date, y = count_pickups_per_day, color = base, group = base)) +
    geom_point(size = 2) +  # Use points to mark zero pickups
    labs(title = title,
         x = "Date",
         y = "Zero Pickups",
         color = "Base") +
    theme_minimal() +
    theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
    scale_y_continuous(limits = c(0, 0.1), expand = c(0, 0))  # Fix y-axis to a small range to emphasize 0 pickups
}
```

```{r}
# Apply the function to your dataset
plot_zero_pickups(train_filtered_1000m_data, "Days with Zero Uber Pickups per Base (1000m Data)")
```

```{r}
# Apply the function to your dataset
plot_zero_pickups(train_filtered_2000m_data, "Days with Zero Uber Pickups per Base (2000m Data)")
```

For both 1000m and 2000m data:
* Base B02764 and B02512 have a significantly lower number of Uber pickups compared to the other bases. This pattern is consistent in both datasets.

* Despite the lower overall pickup counts, there is no day in the data where no Uber pickups occur from any of the bases. Each base, including B02764 and B02512, consistently records pickups on every day within the date range. This indicates that all bases are operational throughout the dataset, even though the pickup activity differs in scale.

* Given the consistent activity from all bases, we conclude that all bases are relevant to our analysis. We will therefore retain all records from all bases to ensure the model captures the entire range of Uber activity across the city.

```{r}
train_filtered_1000m_data
```

```{r}
train_filtered_2000m_data
```

### Remove Unnecessary Features
In this step, we are removing unnecessary features from both the 1000m and 2000m datasets to streamline the data for modeling. Specifically, we are removing the timestamp, lat, lon, base, and dist columns, which are not needed for our predictive models. Our goal is to retain only the time_interval and number_of_pickups columns. Additionally, we ensure that each time interval appears only once (without repeats), as duplicates can distort predictions.
```{r}
# Function to remove unnecessary features and keep unique time intervals
clean_data <- function(data) {
  data %>%
    select(time_interval, number_of_pickups) %>%  # Keep only relevant columns
    distinct(time_interval, .keep_all = TRUE)  # Ensure no duplicate time intervals
}
```

```{r}
# Apply the function to the 1000m dataset
train_filtered_1000m_data <- clean_data(train_filtered_1000m_data)
train_filtered_1000m_data
```

```{r}
# Apply the function to the 2000m dataset
train_filtered_2000m_data <- clean_data(train_filtered_2000m_data)
head(train_filtered_2000m_data)
```

## Part B: Exploratory Data Analysis (EDA)
### 1. Data Preparation
#### Date base data: Adding Columns
We added in the Data Rearrangement part:
1. number_of_pickups: this is how many pickups were is this specific 15-min interval (the label).

2. dist (for the 1000m data): this is the distance from each pickup to the Empire State Building

We will also add:
1.is_weekend: Extract whether the pickup occurred on a weekday or weekend. This is important because pickup patterns often differ on weekdays versus weekends.

2. time_of_day: Group pickup times into time categories such as morning, afternoon, evening, and night to capture demand patterns.

3. Dummy of the Bases: Convert the categorical base variable into dummy (one-hot encoded) variables.

Add The is_weekend Feature:
```{r adding_is_weekend}
# Function to add 'is_weekend' feature using time_interval
add_is_weekend <- function(data) {
  data %>%
    mutate(
      is_weekend = ifelse(wday(time_interval, label = TRUE) %in% c("Sat", "Sun"), 1, 0)  # Check if it's a weekend (Saturday or Sunday)
    )
}
```

```{r}
# Apply the function to the 1000m dataset
train_filtered_1000m_data <- add_is_weekend(train_filtered_1000m_data)
train_filtered_1000m_data
```

```{r}
# Apply the function to the 2000m dataset
train_filtered_2000m_data <- add_is_weekend(train_filtered_2000m_data)
train_filtered_2000m_data
```

Add The time_of_day Feature (in both categorical form & dummy form):

Note: In this case, since our data contains times only between 17:00 and 00:00, we will adapt the time_of_day feature to reflect only the relevant periods: Evening (17:00–20:30) and Night (20:31–00:00). There will be no Afternoon or Morning periods as the data doesn't cover these hours.

Step 1: Adding time_of_day as a Categorical Feature
This feature assigns a number to each relevant time period of the day. In our case, this includes Evening = 1 and Night = 2. This approach is suitable for tree-based models as these models handle categorical variables natively and can split data based on these categories without assuming any linear relationship.

```{r}
# Function to add 'time_of_day' as a categorical feature
add_categorical_time_of_day <- function(data) {
  data %>%
    mutate(
      time_of_day = case_when(
        hour(time_interval) == 17 ~ 1,                              
        hour(time_interval) > 17 & hour(time_interval) < 20 ~ 1,        
        hour(time_interval) == 20 & minute(time_interval) <= 30 ~ 1,    
        (hour(time_interval) == 20 & minute(time_interval) > 30) |      
        (hour(time_interval) > 20 & hour(time_interval) <= 23) ~ 2,
        hour(time_interval) == 0 ~ 2
      )
    )
}
```

```{r}
# Apply the function to the 1000m dataset
train_filtered_1000m_data <- add_categorical_time_of_day(train_filtered_1000m_data)
train_filtered_1000m_data
```

```{r}
# Apply the function to the 2000m dataset
train_filtered_2000m_data <- add_categorical_time_of_day(train_filtered_2000m_data)
train_filtered_2000m_data
```

Step 2: Adding time_of_day as Dummy Variables (One-Hot Encoding)
* In this approach, we create separate columns for each time period (Afternoon and Evening). Since linear models (e.g., linear regression, logistic regression) expect numeric inputs, this prevents the model from assuming any ordinal relationship between these categories. With dummy variables, each time period is treated independently.

* Note: Later on, we will remove one of the dummy variables to avoid multicollinearity, which can lead to issues when running linear models. However, for now, we will keep both dummy variables as part of our exploratory data analysis (EDA) to fully explore the data.

* Relevant Time Periods:
Evening: 17:00–20:30
Night: 20:31–00:00

Here's the code for adding the time_of_day feature using one-hot encoding, broken down into separate dummy features (Night & Evening):

```{r adding_time_of_day}
# Function to add 'time_of_day' as one-hot encoded dummy variables 
add_dummy_time_of_day <- function(data) {
  data %>%
    mutate(
      # Create dummy variables for time of day
      is_evening = ifelse(
        (hour(time_interval) == 17) | 
        (hour(time_interval) > 17 & hour(time_interval) < 20) | 
        (hour(time_interval) == 20 & minute(time_interval) <= 30), 1, 0
      ),
      is_night = ifelse(
        (hour(time_interval) == 20 & minute(time_interval) > 30) | 
        (hour(time_interval) > 20 & hour(time_interval) <= 23), 1, 0 |
        (hour(time_interval) == 0)
      )
    ) # %>%
    # select(-is_evening)  # Optionally remove one category to avoid the dummy variable trap
}

```

```{r}
# Apply the function to the 1000m dataset
train_filtered_1000m_data <- add_dummy_time_of_day(train_filtered_1000m_data)
train_filtered_1000m_data
```

```{r}
# Apply the function to the 2000m dataset
train_filtered_2000m_data <- add_dummy_time_of_day(train_filtered_2000m_data)
train_filtered_2000m_data
```
TO DEL:
Add the dummy of the bases:
* Similar to the time_of_day feature, we need to convert the categorical base variable into dummy (one-hot encoded) variables when working with linear models. This prevents the model from assuming any ordinal relationship between the base codes. Since we have five base codes (B02598, B02617, B02682, B02764, B02512), we will create dummy variables for each.
* Note: Later on, we will remove one of the dummies to avoid the dummy variable trap (when all dummy features are 0, the removed category is 1). This is important in models like linear regression to prevent multicollinearity. For now, we will keep all dummies in place for the purposes of exploratory data analysis (EDA).

```{r}
# Function to add dummy variables for base codes and remove one dummy to avoid the dummy trap
# add_dummy_bases <- function(data) {
#  data %>%
#    mutate(
#      is_B02598 = ifelse(base == "B02598", 1, 0),
#      is_B02617 = ifelse(base == "B02617", 1, 0),
#      is_B02682 = ifelse(base == "B02682", 1, 0),
#      is_B02764 = ifelse(base == "B02764", 1, 0),
#      is_B02512 = ifelse(base == "B02512", 1, 0)
      # Later on B02512 will be the removed base (if all are 0, it's B02512)
      # %>%
      # select(-is_B02512)
#    )
#}
```

```{r}
# Apply the function to the 1000m dataset
# train_filtered_1000m_data <- add_dummy_bases(train_filtered_1000m_data)
# head(train_filtered_1000m_data)
```

```{r}
# Apply the function to the 2000m dataset
# train_filtered_2000m_data <- add_dummy_bases(train_filtered_2000m_data)
# head(train_filtered_2000m_data)
```

Explanation for Model Usage:
* Tree-Based Models (like decision trees, random forests, etc.) can directly handle the categorical base codes and the time_of_day feature in its original categorical form. These models don't require dummy variables and can split the data based on the base or time of day without assuming any linear relationship.

* Linear Models (like linear regression, logistic regression, etc.) require dummy variables for categorical features like base and time_of_day to avoid false assumptions about relationships between different categories. Dummy variables prevent the model from interpreting categories like B02598 and B02617 as having a linear relationship, which would distort the model.

* Dummy variable trap: When using one-hot encoding, it's common to drop one category (here, is_night & B02512) because its value can be inferred when all other dummy variables are 0. This avoids redundancy and multicollinearity.

By adding both a categorical feature and dummy variables for time_of_day & base, later on we will switch between the two versions depending on the model we will use:
* Categorical feature for tree-based models.
* Dummy variables for linear models.

#### Data from the Web: External Data Integration

Here is a CSV of the history weather bulk for Empire State Building (40.75,-73.99) from January 01, 2014 to December 31, 2014.
* The parameters of the data explained here: https://openweathermap.org/history-bulk

```{r}
# Load the CSV file
weather_data <- read_csv("empire-state-weather-2014.csv")
weather_data
```

```{r}
# Convert the 'dt_iso' column to POSIXct format and create the new 'timestamp' column
weather_data <- weather_data %>%
  mutate(timestamp = as.POSIXct(dt_iso, format = "%Y-%m-%d %H:%M:%S", tz = "UTC"))

# Create 'is_raining_last_hour' and 'is_snowing_last_hour' features
weather_data <- weather_data %>%
  mutate(is_raining_last_hour = ifelse(!is.na(rain_1h) & rain_1h > 0, 1, 0),
         is_snowing_last_hour = ifelse(!is.na(snow_1h) & snow_1h > 0, 1, 0))

weather_data
```

Code to Check Minutes and Seconds in Timestamps:
```{r}
# Filter rows where the minute or second is not 0
non_zero_minute_or_second <- weather_data %>%
  filter(minute(timestamp) != 0 | second(timestamp) != 0)

# Check if there are any rows with non-zero minute or second values
if (nrow(non_zero_minute_or_second) > 0) {
  print("Rows with non-zero minutes or seconds found:")
  print(non_zero_minute_or_second)
} else {
  print("All timestamps have 0 for both minutes and seconds.")
}
```

We have confirmed that the weather data is recorded at exact hourly intervals, meaning the data is consistent without variations in the minutes or seconds. To merge this data with our Uber pickup data, we need to ensure our timestamps align correctly. Since the weather information is relevant for each hour as recorded, we will always round the Uber pickup timestamps down to the current hour to maintain accuracy and ensure a seamless merge.

Additionally, to reflect the hourly nature of the data, we will rename the timestamp column to hourly_time_interval to indicate that it represents the start of each hour.

```{r}
# Rename the 'timestamp' column to 'hourly_time_interval'
weather_data <- weather_data %>%
  rename(hourly_time_interval = timestamp)

# Check the first few rows to verify the change
head(weather_data)
```

We combine weather data (temperature, humidity, wind speed, and snow indication) with the Uber pickup data (1000m and 2000m datasets) using the hourly_time_interval as the key. We round all Uber pickup timestamps (i.e., the time_interval) down to the current hour to match the weather data, which is recorded at one-hour intervals. This approach ensures that the weather conditions are consistently matched with the Uber pickup data at the appropriate hourly time.

We are using a function called rounded_timestamp that adds a new feature to the Uber pickup datasets. This feature rounds the time_interval down to the start of the current hour to align it with the hourly_time_interval in the weather data.
```{r}
# Function to round the time_interval to the current hour
rounded_timestamp <- function(df) {
  df %>%
    mutate(
      rounded_timestamp = as.POSIXct(floor_date(time_interval, "hour"), format="%Y-%m-%d %H:%M:%S", tz = "UTC")
    )
}
```

```{r}
# Round time_interval for 1000m data
train_filtered_1000m_data <- rounded_timestamp(train_filtered_1000m_data)
train_filtered_1000m_data
```

```{r}
# Round time_interval for 2000m data
train_filtered_2000m_data <- rounded_timestamp(train_filtered_2000m_data)
train_filtered_2000m_data
```

```{r}
head(weather_data)
```

Select the Required Columns from the Weather Data:
```{r}
# Select only the required columns from the weather data
weather_data_selected <- weather_data %>%
  select(hourly_time_interval, temp, humidity, wind_speed, feels_like, clouds_all, is_raining_last_hour, is_snowing_last_hour)
```

Merging Weather Data with 1000m and 2000m Data Based on rounded_timestamp and hourly_time_interval:
```{r}
# Merge the weather data with the 1000m dataset using 'rounded_timestamp' and 'hourly_time_interval'
train_filtered_1000m_data <- train_filtered_1000m_data %>%
  left_join(weather_data_selected, by = c("rounded_timestamp" = "hourly_time_interval"))

# Check the first few rows of the updated 1000m dataset
head(train_filtered_1000m_data)
```

```{r}
# Merge the weather data with the 2000m dataset using 'rounded_timestamp' and 'hourly_time_interval'
train_filtered_2000m_data <- train_filtered_2000m_data %>%
  left_join(weather_data_selected, by = c("rounded_timestamp" = "hourly_time_interval"))

# Check the first few rows of the updated 2000m dataset
head(train_filtered_2000m_data)
```

Notes:
* Merging Based on hourly_time_interval: The weather data uses hourly_time_interval as its timestamp, while the Uber data uses rounded_timestamp, which we created to match the hourly granularity of the weather data.

* Preserving All Rows from Uber Data: We use a left_join, which ensures that all rows from train_filtered_1000m_data and train_filtered_2000m_data are preserved. If there is no matching weather data for a given rounded_timestamp, the weather columns will contain NA.

* No Additional Rows from Weather Data: The weather data is not expanded to include extra rows in the Uber pickup datasets. Only the relevant weather columns are added to the existing rows in the Uber data.

Lets check NAs after adding new features:
```{r}
# For 1000m data
nas_1000m <- count_nas_per_column(train_filtered_1000m_data)
print("NA counts for 1000m data:")
print(nas_1000m)
```

```{r}
# For 2000m data
nas_2000m <- count_nas_per_column(train_filtered_2000m_data)
print("NA counts for 2000m data:")
print(nas_2000m)
```
No NAs!

#### Dealing with outliers
```{r outliers}
# not sure if needed here or at all

```

```{r}
# Assuming your table (data frame) is named 'my_table'
# write.csv(train_filtered_1000m_data, "train_filtered_1000m_data.csv")
# write.csv(train_filtered_2000m_data, "train_filtered_2000m_data.csv")
```
### 2. Exploratory Analysis

1. Actions Taken on the Data (Data Cleaning and Transformation):
Start by documenting what actions have been performed on the data:

Filtered Data: Data was filtered based on distance from the Empire State Building:
1. train_raw_data which is now called train_filtered_1000m_data table which has only data about pickups which was occurred up to 1000m radius from the Empire State Building.
2. train_filtered_2000m_data which has data about pickups which was occurred up to 1000m radius from the Empire State Building.

New Variables: The dataset includes additional columns like dist, time_interval, number_of_pickups,	is_weekend,	hour,	time_of_day,	is_morning,	is_afternoon,	is_evening,	is_B02598,	is_B02617,	is_B02682	is_B02764.
Rounded Timestamps: The timestamps were rounded to 15-minute intervals (rounded_timestamp).


#### Descriptive Statistics and Visualization
```{r descriptive_statistics}
# # Calculate summary statistics for the 1000m dataset
# summary_stats_1000m <- train_filtered_1000m_data %>%
#   select(dist, number_of_pickups, hour, is_weekend, is_morning, is_afternoon, is_evening) %>%
#   summary()

# # Print summary statistics for the 1000m dataset
# cat("Summary statistics for 1000m dataset:\n")
# print(summary_stats_1000m)

# Calculate summary statistics for the 2000m dataset
# summary_stats_2000m <- train_filtered_2000m_data %>%
#   select(dist, number_of_pickups, hour, is_weekend, is_morning, is_afternoon, is_evening) %>%
#   summary()

# Print summary statistics for the 2000m dataset
# cat("\nSummary statistics for 2000m dataset:\n")
# print(summary_stats_2000m)

```

```{r}
colnames(train_filtered_1000m_data)
cat("\n")
colnames(train_filtered_2000m_data)

```

```{r}
# # 1. Scatter Plot: Number of pickups vs. Distance (dist)
# ggplot(train_filtered_1000m_data, aes(x = dist, y = number_of_pickups)) +
#   geom_point(color = "blue", alpha = 0.6) +
#   labs(title = "Number of Pickups vs Distance (1000m radius)", 
#        x = "Distance from Empire State Building (m)", 
#        y = "Number of Pickups") +
#   theme_minimal()

# # 2. Boxplot: Number of pickups by Hour of the day
# ggplot(train_filtered_1000m_data, aes(x = factor(hour), y = number_of_pickups)) +
#   geom_boxplot(fill = "orange") +
#   labs(title = "Number of Pickups by Hour (1000m radius)", 
#        x = "Hour of the Day", 
#        y = "Number of Pickups") +
#   theme_minimal()
# 

# Boxplot: Number of Pickups by Weekend/Weekday
ggplot(train_filtered_1000m_data, aes(x = factor(is_weekend), y = number_of_pickups)) +
  geom_boxplot(fill = "purple") +
  labs(title = "Number of Pickups on Weekdays vs Weekends", 
       x = "Is Weekend (1 = Yes, 0 = No)", 
       y = "Number of Pickups") +
  theme_minimal()


# # Scatter Plot: Number of Pickups by Hour for Specific Bases (e.g., B02598)
# ggplot(train_filtered_1000m_data, aes(x = hour, y = number_of_pickups, color = factor(is_B02598))) +
#   geom_point(alpha = 0.6) +
#   labs(title = "Number of Pickups by Hour for Base B02598", 
#        x = "Hour of the Day", 
#        y = "Number of Pickups", color = "Is Base B02598") +
#   theme_minimal()
# 

# # Repeat for the 2000m dataset (similar plots can be created for comparison)
# # 1. Scatter Plot: Number of pickups vs. Distance (dist) for 2000m
# ggplot(train_filtered_2000m_data, aes(x = dist, y = number_of_pickups)) +
#   geom_point(color = "red", alpha = 0.6) +
#   labs(title = "Number of Pickups vs Distance (2000m radius)", 
#        x = "Distance from Empire State Building (m)", 
#        y = "Number of Pickups") +
#   theme_minimal()

# # Boxplot: Number of Pickups by Time of Day
# ggplot(train_filtered_1000m_data, aes(x = factor(time_of_day), y = number_of_pickups)) +
#   geom_boxplot(fill = "purple") +
#   labs(title = "Number of Pickups by Time of Day", 
#        x = "Time of Day (Night = 1, Morning = 2, Afternoon = 3, Evening = 4)", 
#        y = "Number of Pickups") +
#   theme_minimal()



```

```{r}
# # Improved scatter plot: Number of pickups vs. Distance with transparency and smaller points
# ggplot(train_filtered_1000m_data, aes(x = dist, y = number_of_pickups)) +
#   geom_point(color = "blue", alpha = 0.3, size = 1) +  # Adding transparency and reducing size
#   labs(title = "Number of Pickups vs Distance (1000m radius)", 
#        x = "Distance from Empire State Building (m)", 
#        y = "Number of Pickups") +
#   theme_minimal()

# # Alternative: 2D density plot for better clarity
# ggplot(train_filtered_1000m_data, aes(x = dist, y = number_of_pickups)) +
#   geom_bin2d(bins = 30) +  # Use 2D binning to visualize the density of points
#   scale_fill_continuous(type = "viridis") +  # Add color scale for density
#   labs(title = "2D Density Plot: Number of Pickups vs Distance (1000m radius)", 
#        x = "Distance from Empire State Building (m)", 
#        y = "Number of Pickups") +
#   theme_minimal()


# Bar Plot: Average Number of Pickups by Weekday vs Weekend
ggplot(train_filtered_1000m_data, aes(x = factor(is_weekend), y = number_of_pickups)) +
  stat_summary(fun = "mean", geom = "bar", fill = "lightblue") +
  labs(title = "Average Number of Pickups: Weekday vs Weekend", 
       x = "Is Weekend (0 = Weekday, 1 = Weekend)", 
       y = "Average Number of Pickups") +
  theme_minimal()

# Line plot of pickups over time
ggplot(train_filtered_1000m_data, aes(x = time_interval, y = number_of_pickups)) +
  geom_line(color = "blue") +
  labs(title = "Pickups Over Time",
       x = "Time Interval",
       y = "Number of Pickups") +
  theme_minimal()

# # Histogram of pickups by time of day
# ggplot(train_filtered_1000m_data, aes(x = hour)) +
#   geom_histogram(binwidth = 1, fill = "lightblue", color = "black") +
#   labs(title = "Distribution of Pickups by Hour of Day",
#        x = "Hour of the Day",
#        y = "Frequency of Pickups") +
#   theme_minimal()

# # Bar plot: average number of pickups by time of day
# ggplot(train_filtered_1000m_data, aes(x = as.factor(hour), y = number_of_pickups)) +
#   stat_summary(fun = "mean", geom = "bar", fill = "lightgreen") +
#   labs(title = "Average Number of Pickups by Time of Day",
#        x = "Hour of the Day",
#        y = "Average Number of Pickups") +
#   theme_minimal()

# # Box plot of pickups over time
# ggplot(train_filtered_1000m_data, aes(x = as.factor(hour), y = number_of_pickups)) +
#   geom_boxplot(fill = "purple", alpha = 0.5) +
#   labs(title = "Distribution of Pickups by Hour of the Day",
#        x = "Hour of the Day",
#        y = "Number of Pickups") +
#   theme_minimal()


```

```{r}
# # Heatmap: Number of pickups by hour and base
# train_filtered_1000m_data %>%
#   gather(base, value, is_B02598:is_B02764) %>%  # Gather the base columns into a single column
#   filter(value == 1) %>%  # Only include rows where the base is active
#   ggplot(aes(x = factor(hour), y = base)) +
#   geom_tile(aes(fill = number_of_pickups), color = "white") +
#   scale_fill_gradient(low = "lightblue", high = "darkblue") +
#   labs(title = "Heatmap of Pickups by Hour and Base", 
#        x = "Hour of the Day", 
#        y = "Base", 
#        fill = "Number of Pickups") +
#   theme_minimal()
```

```{r}
# # Check if there are any pickups for B02512
# sum(train_filtered_1000m_data$is_B02512 == 1)
# 
# colnames(train_filtered_1000m_data)

```

```{r}
# # Facet Plot: Number of Pickups by Hour, Faceted by Base (with different name for the pivoted column)
# train_filtered_1000m_data %>%
#   pivot_longer(cols = c(is_B02598, is_B02617, is_B02682, is_B02764),  # Use only the available base columns
#                names_to = "base_active", values_to = "value") %>%
#   filter(value == 1) %>%  # Only include rows where the base is active
#   ggplot(aes(x = hour, y = number_of_pickups)) +
#   geom_point(alpha = 0.5, color = "darkgreen") +
#   facet_wrap(~base_active) +  # Create a facet for each active base
#   labs(title = "Distribution of Pickups by Hour, Faceted by Active Base", 
#        x = "Hour of the Day", 
#        y = "Number of Pickups") +
#   theme_minimal()

```

```{r}
# # Map numeric 'time_of_day' values to their respective labels
# train_filtered_1000m_data <- train_filtered_1000m_data %>%
#   mutate(time_of_day_label = case_when(
#     time_of_day == 1 ~ "Night",
#     time_of_day == 2 ~ "Morning",
#     time_of_day == 3 ~ "Afternoon",
#     time_of_day == 4 ~ "Evening",
#     TRUE ~ "Unknown"
#   ))

# # Bar Plot: Pickups by Time of Day and Base
# train_filtered_1000m_data %>%
#   pivot_longer(cols = c(is_B02598, is_B02617, is_B02682, is_B02764), 
#                names_to = "base_active", values_to = "value") %>%
#   filter(value == 1) %>%
#   ggplot(aes(x = time_of_day_label, y = number_of_pickups, fill = base)) +
#   geom_bar(stat = "identity", position = "dodge") +
#   labs(title = "Pickups by Time of Day and Base", 
#        x = "Time of Day", 
#        y = "Number of Pickups", 
#        fill = "Base") +
#   theme_minimal()

```

```{r}
# # Line plot showing average pickups by hour for each base
# train_filtered_1000m_data %>%
#   pivot_longer(cols = c(is_B02598, is_B02617, is_B02682, is_B02764), 
#                names_to = "base_active", values_to = "value") %>%
#   filter(value == 1) %>%
#   group_by(base, hour) %>%
#   summarise(avg_pickups = mean(number_of_pickups)) %>%
#   ggplot(aes(x = hour, y = avg_pickups, color = base)) +
#   geom_line(size = 1) +
#   labs(title = "Average Number of Pickups by Hour and Base", 
#        x = "Hour of the Day", 
#        y = "Average Number of Pickups", 
#        color = "Base") +
#   theme_minimal()

```

```{r}
# Boxplot: Number of Pickups on Weekdays vs Weekends
ggplot(train_filtered_1000m_data, aes(x = factor(is_weekend), y = number_of_pickups, fill = factor(is_weekend))) +
  geom_boxplot() +
  labs(title = "Number of Pickups: Weekdays vs Weekends", 
       x = "Is Weekend (0 = Weekday, 1 = Weekend)", 
       y = "Number of Pickups", 
       fill = "Weekend Indicator") +
  theme_minimal()

```

```{r}
# Check the range of the 'rounded_timestamp' column
range(train_filtered_1000m_data$rounded_timestamp)

```

```{r}
# Time Series: Pickups over time
ggplot(train_filtered_1000m_data, aes(x = rounded_timestamp, y = number_of_pickups, group = 1)) +
  geom_line(color = "blue", size = 1) +
  labs(title = "Time Series of Pickups Throughout the Day", 
       x = "Time (15-minute intervals)", 
       y = "Number of Pickups") +
  theme_minimal()

```

```{r}
# # Group data by day and hour to find average pickups at different times of the day
# daily_patterns <- train_filtered_1000m_data %>%
#   mutate(date = as.Date(rounded_timestamp)) %>%
#   group_by(date, hour) %>%
#   summarise(avg_pickups = mean(number_of_pickups)) %>%
#   ungroup()
# 
# # Visualize average pickups per hour over days
# ggplot(daily_patterns, aes(x = hour, y = avg_pickups, group = date, color = date)) +
#   geom_line() +
#   labs(title = "Average Daily Pickup Pattern by Hour", 
#        x = "Hour of the Day", 
#        y = "Average Number of Pickups") +
#   theme_minimal()
```

```{r}
# # Add a column to indicate whether the day is a weekend
# train_filtered_1000m_data <- train_filtered_1000m_data %>%
#   mutate(weekend = ifelse(weekdays(as.Date(rounded_timestamp)) %in% c("Saturday", "Sunday"), "Weekend", "Weekday"))
# 
# # Group by hour and weekend/weekday to analyze patterns
# weekend_patterns <- train_filtered_1000m_data %>%
#   group_by(weekend, hour) %>%
#   summarise(avg_pickups = mean(number_of_pickups)) %>%
#   ungroup()

# # Visualize patterns for weekdays and weekends
# ggplot(weekend_patterns, aes(x = hour, y = avg_pickups, color = weekend)) +
#   geom_line(size = 1) +
#   labs(title = "Pickup Patterns: Weekdays vs Weekends", 
#        x = "Hour of the Day", 
#        y = "Average Number of Pickups") +
#   theme_minimal()

```

```{r}
# # Install necessary package for rolling window calculations
# install.packages("zoo")
# library(zoo)
# 
# # Compute a 7-day rolling mean and standard deviation (assuming 96 15-minute intervals per day)
# train_filtered_1000m_data <- train_filtered_1000m_data %>%
#   arrange(rounded_timestamp) %>%
#   mutate(rolling_mean = rollmean(number_of_pickups, k = 96*7, fill = NA),  # 7-day window
#          rolling_sd = rollapply(number_of_pickups, width = 96*7, FUN = sd, fill = NA))
# 
# # Plot the rolling mean and rolling standard deviation without saving
# ggplot(train_filtered_1000m_data, aes(x = rounded_timestamp)) +
#   geom_line(aes(y = rolling_mean), color = "blue", size = 1) +
#   geom_line(aes(y = rolling_sd), color = "red", linetype = "dashed", size = 0.8) +
#   labs(title = "Rolling Mean and Standard Deviation of Pickups", 
#        x = "Date", 
#        y = "Pickups (7-day Rolling Mean and SD)") +
#   theme_minimal()
```

TO Delete ---->
Breakdown of What You're Seeing:
7-Day Rolling Mean (Blue Line):

This smooths out daily fluctuations by averaging the data over a 7-day window. It helps identify long-term trends in the data, while ignoring short-term noise.
The peaks in the blue line indicate periods where the average number of pickups was higher, while the troughs show periods of lower average pickups.
The pattern you are seeing here appears to be cyclical, with higher pickup activity on certain days of the week (likely weekdays, driven by commuting patterns).
7-Day Rolling Standard Deviation (Red Line):

The dashed red line represents the rolling standard deviation over the same 7-day window. Standard deviation is a measure of volatility or variability - the higher the value, the more spread out the number of pickups are during that window.
When the red line spikes, it means that the number of pickups was more volatile, meaning the numbers fluctuated more during that period.
When the red line is lower, it indicates a more stable period, where the number of pickups was relatively consistent.

Observations:
Cyclic Nature: There is a repeating pattern of rising and falling rolling means. This suggests a weekly pattern in Uber demand, where demand is likely to be higher on certain days of the week, and lower on others (e.g., weekends).
Higher Variability Around Peaks: You can observe that the red line (rolling standard deviation) tends to increase during periods where the blue line (rolling mean) spikes. This indicates that variability is higher when demand increases. In other words, during peak periods, there is greater unpredictability or fluctuation in the number of pickups.
Stable Low Periods: The rolling standard deviation drops during periods of low demand (when the blue line dips). This suggests that low-demand periods are generally more consistent and less variable.


```{r}
# Plot lag relationship between pickup counts at different time lags
par(mfrow = c(2, 3))  # Set up the plotting area for 6 plots (2 rows, 3 columns)
for (i in 1:6) {
  lag.plot(train_filtered_1000m_data$number_of_pickups, lags = i, main = paste("Lag Plot with Lag", i))
}

```
Strong Positive Correlation:

The points in the lag plot are closely aligned along a diagonal line, indicating a strong positive correlation between the number of pickups at time t and the number of pickups at time t-1 (one lag prior).
This means that the number of pickups at one time step is highly influenced by the number of pickups in the previous time step. If there were many pickups at time t-1, there will likely be a similar number of pickups at time t.

Insights:
Predictability: Since there is a strong positive correlation between current and previous pickups, this suggests that the number of pickups is highly predictable based on recent data. A time series model like ARIMA or Exponential Smoothing should perform well in forecasting future pickup trends.
Lagged Effects: The correlation across time steps indicates that demand cycles are quite smooth and continuous, with pickup numbers transitioning gradually from one time step to the next rather than fluctuating wildly.


## Part C: Forecast for the Future

### Model 1 - Full Data
Final Data for Modeling:
Since the test data doesn't provide exact pickup coordinates (latitude and longitude), but all pickups are known to occur within a 1000-meter radius from the Empire State Building, it's essential to ensure that the final training dataset only includes pickups within this same 1000-meter radius. Therefore, we must filter the training data accordingly. Additionally, because the test data lacks latitude, longitude, and distance features, it's important to remove these features from the final training dataset before modeling. This step ensures consistency between the training and test datasets, enabling the model to make accurate predictions.

```{r fix_train_data_for_model}

```

```{r fix_test_data_for_model}

```

```{r model_1_full_data}
# קוד לאימון המודל הראשון באמצעות הנתונים המלאים
```

### Model 2 - Filtered Data
1. The Problem:
The training data includes Uber pickups within a 2000-meter radius, while the test data represents pickups within a 1000-meter radius around the Empire State Building. This difference in radius creates a problem because the model might learn patterns from a wider area (2000m) that do not apply to the narrower, more localized 1000m area required for the predictions in the test data.

2. Clustering Solution:
Since the test data doesn't include specific locations (latitude, longitude), and the cluster based on location from the test data is not present in the training data, we need a non-location-based clustering approach.

```{r fix_train_data_for_model}

```

```{r fix_test_data_for_model}

```

```{r model_2_filtered_data}
# קוד לאימון המודל השני באמצעות הנתונים המסוננים (מחוץ לרדיוס 2000 מטר)
```

## Conclusion
```{r conclusion}
# סיכום הממצאים והתובנות מהמודלים
```